{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbasecondabf8fb41153a84f57a80aba5a735c6d6d",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import seaborn as sns\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata = pd.read_csv('data/top10songslyrics.csv')\n",
    "lyricaldata = songdata[songdata['Lyrics'].notnull()].reset_index(drop=True) # Make a new dataframe that contains only non-instrumentals, and reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing to prepare for sentiment analysis.\n",
    "\n",
    "for i in range(0,len(lyricaldata)):\n",
    "    lyrics = lyricaldata.loc[i,'Lyrics']\n",
    "    lyrics = re.sub(r'[Gg]onna','going to',lyrics)\n",
    "    lyrics = re.sub(r'[Ww]anna','want to',lyrics)\n",
    "    lyrics = re.sub(r'[Yy]\\'all','you all',lyrics)\n",
    "    lyrics = re.sub(r'\\'cause','because',lyrics)\n",
    "    lyrics = re.sub(r'[Gg]otta','have to',lyrics)\n",
    "    lyrics = re.sub(r'gon\\'','going to',lyrics)\n",
    "    lyrics = re.sub(r'in\\'','ing',lyrics)\n",
    "    lyrics = re.sub(r'ingt','in\\'t',lyrics) # Fix 'ain't'.\n",
    "    lyrics = re.sub(r'\\'em','them',lyrics)\n",
    "    lyricaldata.loc[i,'Lyrics'] = lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(lyricaldata)):\n",
    "    my_text = lyricaldata.loc[i,'Lyrics']\n",
    "    token_text = word_tokenize(my_text)\n",
    "    token_text = [l.lower() for l in token_text if not re.fullmatch('[' + string.punctuation + ']+', l)] #Remove tokens that are just punctuation.\n",
    "    port_stem = PorterStemmer() # The following code will stem the words in the lyrics. This means that, for example, \"stop\", \"stopping\", and \"stops\" will only count as one unique word.\n",
    "    stem_text = []\n",
    "    for k in range(len(token_text)):\n",
    "        stem_text.append(port_stem.stem(token_text[k]))\n",
    "    lyricaldata.loc[i,'Unique Words'] = len(set(stem_text)) # Redoing the unique/total words here after data cleaning.\n",
    "    lyricaldata.loc[i,'Total Words'] = len(token_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyricaldata['Uniqueness'] = lyricaldata['Total Words']/lyricaldata['Unique Words'] # This metric is the amount of times the average word in a song is repeated.\n",
    "\n",
    "lyricaldata['Positive'] = 0\n",
    "lyricaldata['Neutral'] = 0\n",
    "lyricaldata['Negative'] = 0\n",
    "trueneutral = []\n",
    "\n",
    "for i in range(0,len(lyricaldata)):\n",
    "    lyricaldata.loc[i,'Lyrics'] = re.sub(r'in\\'','ing',lyricaldata.loc[i,'Lyrics'])\n",
    "    token_text = regexp_tokenize(lyricaldata.loc[i,'Lyrics'],pattern=r'\\w[^\\n]*') # Split lyrics by line for sentiment analysis.\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    neu = 0\n",
    "    length = 0\n",
    "    for sentence in token_text:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        if vs['neu'] != 1:\n",
    "            pos += float(vs['pos'])\n",
    "            neg += float(vs['neg'])\n",
    "            neu += float(vs['neu'])\n",
    "            length += 1\n",
    "    try:\n",
    "        lyricaldata.loc[i,'Positive'] = pos/length\n",
    "        lyricaldata.loc[i,'Neutral'] = neu/length\n",
    "        lyricaldata.loc[i,'Negative'] = neg/length\n",
    "    except ZeroDivisionError:\n",
    "        print('Song ' + lyricaldata.loc[i,'Title'] + ' at position ' + str(i) + ' returned neutral.') # This only happens if Vader can't detect sentiments in any line of the song.\n",
    "        trueneutral.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code block generates a dataframe containing the average uniqueness, positive sentiment, neutral sentiment, and negative sentiment values for each year, weighted by how long each individual song spent in the top ten. This essentially reconstructs every individual Billboard Hot 100 chart and evaluates them - i.e., if a song appeared in the top ten on three charts, it is counted three times.\n",
    "\n",
    "years = []\n",
    "unqmeans = []\n",
    "posmeans = []\n",
    "neumeans = []\n",
    "negmeans = []\n",
    "for i in range(lyricaldata['Year'].min(),lyricaldata['Year'].max()+1):\n",
    "    years.append(i)\n",
    "    unqmeans.append(sum(lyricaldata[lyricaldata['Year'] == i]['Uniqueness']*lyricaldata.loc[lyricaldata['Year'] == i,'Number of weeks in top ten'])/sum(lyricaldata[lyricaldata['Year'] == i]['Number of weeks in top ten']))\n",
    "    posmeans.append(sum(lyricaldata[lyricaldata['Year'] == i]['Positive']*lyricaldata.loc[lyricaldata['Year'] == i,'Number of weeks in top ten'])/sum(lyricaldata[lyricaldata['Year'] == i]['Number of weeks in top ten']))\n",
    "    neumeans.append(sum(lyricaldata[lyricaldata['Year'] == i]['Neutral']*lyricaldata.loc[lyricaldata['Year'] == i,'Number of weeks in top ten'])/sum(lyricaldata[lyricaldata['Year'] == i]['Number of weeks in top ten']))\n",
    "    negmeans.append(sum(lyricaldata[lyricaldata['Year'] == i]['Negative']*lyricaldata.loc[lyricaldata['Year'] == i,'Number of weeks in top ten'])/sum(lyricaldata[lyricaldata['Year'] == i]['Number of weeks in top ten']))\n",
    "weightedmeandata = pd.DataFrame(columns=['Uniqueness','Positive','Neutral','Negative'],index=years)\n",
    "weightedmeandata['Uniqueness'] = unqmeans\n",
    "weightedmeandata['Positive'] = posmeans\n",
    "weightedmeandata['Neutral'] = neumeans\n",
    "weightedmeandata['Negative'] = negmeans\n",
    "weightedmeandata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdata = weightedmeandata.drop('Uniqueness',axis=1).drop('Neutral',axis=1)\n",
    "sentdata['Year']=sentdata.index\n",
    "sns.set()\n",
    "with sns.axes_style('white'):\n",
    "    g = sns.FacetGrid(sentdata,height=4,aspect=1.5)\n",
    "sns.despine(left=True,bottom=True)\n",
    "g.map(sns.regplot,'Year','Positive',color='#4c72FF',truncate=True,order=3)\n",
    "g.map(sns.regplot,'Year','Negative',color='#F03600',truncate=True,order=3)\n",
    "plt.ylabel('Sentiment Value',fontsize=18)\n",
    "plt.title('Sentiment Analysis on Lyrics of All Top 10 Songs',fontsize=18)\n",
    "plt.xlabel('')\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.vlines(x=range(1960,2020,5),ymin=weightedmeandata['Negative'].min(),ymax=weightedmeandata['Positive'].max(),alpha=0.1)\n",
    "plt.figtext(0.99, 0.01, 'Generated with Vader Sentiment Analysis\\nDoes not include lyrics with no sentiment detected', fontsize=10,horizontalalignment='right',**{'fontname':'Arial'})\n",
    "plt.annotate('Positive',xy=(2019+0.5,sentdata.loc[2019,'Positive']+0.005),fontsize=15,color='#4c72FF')\n",
    "plt.annotate('Negative',xy=(2019+0.5,sentdata.loc[2019,'Negative']-0.005),fontsize=15,color='#F03600')\n",
    "g.savefig('visualizations/sentiment.png',dpi=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word repetition plot\n",
    "\n",
    "sns.set()\n",
    "sns.set_style('white')\n",
    "plot = sns.regplot(x=list(weightedmeandata.index),y='Uniqueness',data=weightedmeandata)\n",
    "sns.despine(left=True,bottom=True)\n",
    "plt.xlabel('',fontsize=13)\n",
    "plt.ylabel('Avg Repetitions per Word',fontsize=15)\n",
    "plt.title('Repetitiveness of Top 10 Songs Over Time',fontsize=15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.figtext(0.99, 0.01, 'Songs are grouped by year and weighted by how long they stayed in the top 10', fontsize=9.5,horizontalalignment='right',**{'fontname':'Arial'})\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plot.get_figure().savefig('visualizations/weightedrepetition.png',dpi=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1958 is excluded from the following analysis because the Billboard Hot 100 only existed August-December of that year.\n",
    "\n",
    "top10songs = []\n",
    "years = []\n",
    "for i in range(lyricaldata['Year'].min()+1,lyricaldata['Year'].max()+1):\n",
    "    years.append(i)\n",
    "    top10songs.append(len(lyricaldata[lyricaldata['Year'] == i]))\n",
    "top10songsdf = pd.DataFrame(columns=['Number of Top 10 Songs'],index=years)\n",
    "top10songsdf['Number of Top 10 Songs'] = top10songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Songs Per Year plot\n",
    "\n",
    "sns.set()\n",
    "sns.set_style('white')\n",
    "plot = sns.lineplot(data=top10songsdf,palette=sns.set_palette(['#4c72FF']))\n",
    "plot.get_legend().remove()\n",
    "sns.despine(left=True,bottom=True)\n",
    "plt.scatter(1991,top10songsdf.loc[1991,'Number of Top 10 Songs'],color='black',s=20)\n",
    "plt.annotate('[1]',xy=(1991+0.5,top10songsdf.loc[1991,'Number of Top 10 Songs']),fontsize=10)\n",
    "plt.xlabel('',fontsize=13)\n",
    "plt.ylabel('Songs',fontsize=13)\n",
    "plt.title('Songs Reaching the Top 10 Each Year',fontsize=14,**{'fontname':'Helvetica'})\n",
    "plt.xticks(fontsize=12,**{'fontname':'Arial'})\n",
    "plt.yticks(fontsize=12,**{'fontname':'Arial'})\n",
    "plt.figtext(0.99, 0.01, '[1] November 31st, 1991: Billboard Hot 100 begins new data tracking method', fontsize=9,horizontalalignment='right',**{'fontname':'Arial'})\n",
    "plot.get_figure().savefig('visualizations/top10songsperyear.png',dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next four cells generate and then save the wordcloud. Step one: Throw all the text into a bucket. Step two: Add la las, na nas, and bebopdoopetywhops to stopwords. Step three: ??? Step four: Wordcloud.\n",
    "\n",
    "fulltext = ' '.join(i for i in lyricaldata['Lyrics'])\n",
    "fulltext = re.sub(r'(\\n|\\r)',' ',fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update(['got','oh','ooh','na','la','doo','now'])\n",
    "wordcloud = WordCloud(stopwords=stopwords,width=1920,height=1080,max_words=300,background_color='white',max_font_size=600,colormap='viridis',collocations=False).generate(fulltext.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file(\"visualizations/overall_wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = pd.Series(fulltext.lower().split(' ')).value_counts().to_frame()\n",
    "print(wordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're going to generate the barplot showing word frequency distribution.\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update(['got','oh','ooh','na','la','doo','now'])\n",
    "wordfilter = wordcounts.filter(items=stopwords,axis=0)\n",
    "wordcounts['Word'] = wordcounts.index\n",
    "wordcounts = wordcounts.merge(wordfilter,how='left',indicator=True)\n",
    "wordcounts = wordcounts[wordcounts['_merge'] != 'both'][1:].drop('_merge',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts.columns = ['Count','Word']\n",
    "wordcounts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "plt.style.use('seaborn')\n",
    "plot = sns.barplot(x='Word',y='Count',data=wordcounts[0:15])\n",
    "plot.set_xticklabels(plot.get_xticklabels(),rotation=45,fontsize=13)\n",
    "plot.set_yticklabels(range(0,14001,2000),fontsize=13)\n",
    "plt.xlabel('Word',fontsize=18)\n",
    "plt.ylabel('Frequency',fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plot.get_figure().savefig('visualizations/top10songswordsbarplot.png',dpi=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}